<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<title>assignment2</title>
		<style>
			body { margin: 0; }
		</style>
    <script type="importmap">
        {
          "imports": {
            "three": "https://unpkg.com/three@0.147.0/build/three.module.js",
            "three/addons/": "https://unpkg.com/three@0.147.0/examples/jsm/"
          }
        }
    </script>
	</head>
	<body>
		<h1 style="text-align: center;">Assignment 2</h1>
		<h2>Introduction</h2>
		<p>For this assignment, you will be implementing Structure from Motion. 
			<ul>
				<li>Feature Matching</li>
				<li>Structure from Motion</li>
			</ul>
			We have made available a visualization tool using the Three.js library implemented in "./js/assignment2.js" and an example implementation located in "./assignments/assignment2.py". Your objective is to fill in TODOs in the python files and make modification based on it. You are encouraged to use a programming language with which you are comfortable. The output results should be in the ply format, and you must visualize your outcomes accordingly. 
			<br><br>
			<b>How to Submit: </b>Please submit this template file along with your implementation as a zip file. The zip file should contain your source code, the generated results in PLY mesh format, and a report that has been modified using this HTML file. The report should comprise your results and a concise explanation of your implementation. Alternatively, you may choose to create a GitHub repository containing all these elements and provide a link for submission.
			<br><br>
			<b>Requirements / Rubric: </b>The grading is based on the correctness of your implementation. You are encouraged to use the visualization tool to debug your implementation. You can also use the visualization tool to test your implementation on other 3D models. </p>
				<ul>
					<li>+80 pts: Implement the structure-from-motion algorithm with the start code.  </li>
					<li>+20 pts: Write up your project, algorithms, reporting results (reprojection error) and visualisations, compare your reconstruction with open source software Colmap.</li>
					<li>+10 pts: Extra credit (see below)</li>
					<li>-5*n pts: Lose 5 points for every time (after the first) you do not follow the instructions for the hand in format</li>
				</ul>
			<b>Extract Credit:</b> You are free to complete any extra credit:
				<ul>
					<li>up to 5 pts: Present results with your own captured data.</li>
					<li>up to 10 pts: Implement Bundle Adjustment in incremental SFM.</li>
					<li>up to 10 pts: Implement multi-view stereo (dense reconstruction).</li>
					<li>up to 20 pts: Create mobile apps to turn your SFM to a scanner.</li>
					<li>up to 10 pts: Any extra efforts you build on top of basic SFM.</li>
				</ul>
		</p>
		<h2>My Results</h2>
		<p>You can run the code by typing <code>python assignments/assignment2/assignment2.py</code> or separately <code>python assignments/assignment2/feat_match.py</code> for feature matching and <code>python assignments/assignment2/sfm.py</code> for the whole structure-from-motion pipeline.</p>
		<p>For base credits, I implemented the structure-from-motion algorithm with the provided starter code, filling all TODOs in the code.</p>
		<p>Here are the comparisons of my results with the COLMAP results. For the reconstructed ply files, you can check in <code>./assets/assignment2/&lt;dataset_name&gt;.ply</code>.</p>
		<table>
			<tr>
				<th>Dataset</th>
				<th>Projection Errors for each image</th>
				<th>Point cloud result</th>
				<th>Camera result</th>
				<th>COLMAP result</th>
			</tr>
			<tr>
				<td>castle-P19</td>
				<td><img src="/images/castle-P19_errors.png" style="width: 100%;"></td>
				<td><img src="/images/castle-P19_sfm.gif" style="width: 100%;"></td>
				<td><img src="/images/castle-P19_cam.gif" style="width: 100%;"></td>
				<td><img src="/images/castle-P19_colmap.gif" style="width: 100%;"></td>
			</tr>
			<tr>
				<td>castle-P30</td>
				<td><img src="/images/castle-P30_errors.png" style="width: 100%;"></td>
				<td><img src="/images/castle-P30_sfm.gif" style="width: 100%;"></td>
				<td><img src="/images/castle-P30_cam.gif" style="width: 100%;"></td>
				<td><img src="/images/castle-P30_colmap.gif" style="width: 100%;"></td>
			</tr>
			<tr>
				<td>entry-P10</td>
				<td><img src="/images/entry-P10_errors.png" style="width: 100%;"></td>
				<td><img src="/images/entry-P10_sfm.gif" style="width: 100%;"></td>
				<td><img src="/images/entry-P10_cam.gif" style="width: 100%;"></td>
				<td><img src="/images/entry-P10_colmap.gif" style="width: 100%;"></td>
			</tr>
			<tr>
				<td>fountain-P11</td>
				<td><img src="/images/fountain-P11_errors.png" style="width: 100%;"></td>
				<td><img src="/images/fountain-P11_sfm.gif" style="width: 100%;"></td>
				<td><img src="/images/fountain-P11_cam.gif" style="width: 100%;"></td>
				<td><img src="/images/fountain-P11_colmap.gif" style="width: 100%;"></td>
			</tr>
			<tr>
				<td>Herz-Jesus-P8</td>
				<td><img src="/images/Herz-Jesus-P8_errors.png" style="width: 100%;"></td>
				<td><img src="/images/Herz-Jesus-P8_sfm.gif" style="width: 100%;"></td>
				<td><img src="/images/Herz-Jesus-P8_cam.gif" style="width: 100%;"></td>
				<td><img src="/images/Herz-Jesus-P8_colmap.gif" style="width: 100%;"></td>
			</tr>
			<tr>
				<td>Herz-Jesus-P25</td>
				<td><img src="/images/Herz-Jesus-P25_errors.png" style="width: 100%;"></td>
				<td><img src="/images/Herz-Jesus-P25_sfm.gif" style="width: 100%;"></td>
				<td><img src="/images/Herz-Jesus-P25_cam.gif" style="width: 100%;"></td>
				<td><img src="/images/Herz-Jesus-P25_colmap.gif" style="width: 100%;"></td>
			</tr>
		</table>

		<h3>Extra Credit - My Dataset</h3>
		<p>I captured this video data at the Petersen Automotive Museum. It is me holding a phone and capturing this car from different angles. The original data is a video. I extracted 10 images per second from the video to generate a dataset of 50 pictures.</p>
		<table>
			<tr>
				<td><img src="/images/data_exp1.jpg" style="width: 100%;"></td>
				<td><img src="/images/data_exp2.jpg" style="width: 100%;"></td>
			</tr>
			<tr>
				<td>Example data 1</td>
				<td>Example data 2</td>
			</tr>
		</table>
		<table>
			<tr>
				<td><img src="/images/Mine_errors.png" style="width: 100%;"></td>
				<td><img src="/images/Mine_sfm.gif" style="width: 100%;"></td>
			</tr>
			<tr>
				<td>Projection Errors for each image</td>
				<td>Point cloud demonstration</td>
			</tr>
		</table>

		<h3>Extra Credit - Post-Processing for Point Cloud</h3>
		<p>I noticed that the point cloud generated by the original algorithm had some outliers. I tried using RANSAC to remove the outliers, but it didn't improve the results. Therefore, I adopted a post-processing approach for the point cloud. This function uses the k-nearest neighbors (k-NN) algorithm to remove outliers from the point cloud. For each point, it calculates the average distance to its k nearest neighbors. Then, based on the median of these average distances, a threshold is determined. If a point's average distance to its k nearest neighbors is greater than this threshold multiplied by a threshold_factor, the point is considered an outlier and removed from the point cloud. The purpose of this method is to remove points that are far away from the majority, thereby reducing noise and improving the quality of the point cloud. The visualized results in the demonstration are after removing the outliers using this algorithm. The parameters I used were k=10 and threshold_factor=5. The relevant code can be found in <code>post.py</code>.</p>

		<h3>Extra Credit - Bundle Adjustment</h3>
		<p>There are four steps in bundle adjustment:</p>
		<ol>
			<li><strong>Parameter Preparation:</strong> Integrating the camera's rotation and translation parameters along with the coordinates of the 3D points into a parameter vector.</li>
			<li><strong>Error Function:</strong> Defining an error function that calculates the discrepancy between all 2D points and their corresponding 3D points projected through the camera parameters.</li>
			<li><strong>Optimization Process:</strong> Using a non-linear least squares optimization algorithm to adjust the camera parameters and 3D point coordinates to minimize the total reprojection error.</li>
			<li><strong>Parameter Update:</strong> After the optimization, extracting the updated camera parameters and 3D point coordinates and updating the original parameters.</li>
		</ol>
		<p>In <code>sfm.py</code>, the bundle adjustment can be used with the option <code>--bundle_adjustment true</code>. Suggent do not run it since it takes too long to run scipy.optimize</p>

        <!-- <div id="container"></div>
		<script type="module" src="../js/assignment2.js"></script> -->
	</body>
</html>